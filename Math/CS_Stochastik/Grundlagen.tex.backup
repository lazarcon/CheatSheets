\section{Begriffe}

\begin{description}\itemsep0em
	\item [Deskriptive Statistik] 
	analysiert Messreihen (qualitiv, quantitativ) (Beschäftigt sich mit Stichproben)
	
	\item [Schliessende Statistik] 
	zieht Rückschlüsse von einer Stichprobe auf die Gesamtheit auf Grund der Verteilungsfunktion $P(E)$
	
	\item [Zufallsexperiment] 
	Ein Zufallsexperiment/-versuch ist ein (theoretisch) beliebig oft wiederholbarer Versuch, dessen Ausgang nicht vorhersagbar ist.
	
	Hat ein Versuch $n$ verschiedene Ausgänge, die alle gleich wahrscheinlich sind, so tritt jeder Ausgang $\omega_i$ mit einer Wahrscheinlichkeit von $P(\omega_i) = \frac{1}{n}$ ein

	\item [Elementarereignis] 
	Realistation eines Zufallsexperiments. Also ein einzelnes Ergebnis eines Zufallsexperiments ($\omega$).
	
	\item [Ereignisraum] 
	Menge aller möglichen Ergebnisse eines Zufallversuchs ($\Omega$)
	
	\item [Ereignis] 
	Beliebige Teilmenge $A$ aus $\Omega$. Alle möglichen Ereignisse sind $\mathcal{P}(\Omega)$.
	
	Ein Ereignis, dass $k$ verschiedene Ausgänge umfasst hat die Wahrscheinlichkeit $\frac{k}{n}$.

	\item [Komplementarereignis] 
	Menge aller Elementarereignisse $\overline{A}$, die zum Ereignisraum aber nicht zum Ereignis $A$ gehören: $P(\overline{A}) = 1 - P(A)$
	
	\item [Absolute Häufigkeit] 
	Wie häufig ein Ereignis $E$ in einer Versuchsreihe aus $n$ Versuchen aufgetreten ist ($H_n(E)$)
	
	\item [Relative Häufigkeit] 
	Wie häufig ein Ereignis $E$ im Verhältnis zu anderen Ereignissen aufgetreten ist ($R_n(E) = H_n(E)/n$). 

	Es gilt: $R_n(E) \approx P(E)$ für $n \rightarrow \infty$ Wobei $0 \leq R_n(E) \leq 1$ und $R_n(\Omega) = 1$

	\item [Zufallsvariable]
	($X$) lässt sich formal als Funktion beschreiben, die den Ergebnissen eines Zufallsexperiments Werte (Realisierungen) zuordnet: $X = X(\omega)$

\end{description}

\subsection{Wahrscheinlichkeit (Laplace)}
\begin{equation*}
	P(A) = \frac{\mbox{Zahl der günstigen Fälle}}{\mbox{Zahl der möglichen Fälle}} = \sum_{\omega_i \in A \subset \mathcal{P}} P(\{\omega_i\}) = \frac{|A|}{|\Omega|}
\end{equation*}
gilt nur für endliche Anzahl von Elementareignissen, die alle gleich wahrscheinlich sind! 

\subsection{Statistische Wahrscheinlichkeit (Mises)}
\begin{equation*}
	P(A) = \lim_{n \rightarrow \infty} R_n(A) = \lim_{n \rightarrow \infty} \frac{H_n(A)}{n}
\end{equation*}


\subsubsection{Sätze}
\settowidth{\MyLenA}{Wenn $A$, dann auch $B$ ~~}
\begin{tabular}{@{}p{\the\MyLenA}%
				@{}p{(\linewidth - \the\MyLenA)}}
	Gleichwertigkeit & $(A \wedge B) \vee (\neg A \wedge \neg B) \Leftrightarrow A = B$\\
	Gleichzeitigkeit & $A \wedge B \Leftrightarrow A \cap B$\\
	Komplement & $(A \wedge \neg B) \vee (\neg A \wedge B) \Leftrightarrow A = \overline{B} \Leftrightarrow B = \overline{A}$\\
	$A$ und/oder $B$ & $A \vee B \Leftrightarrow A \cup B$\\
	Wenn $A$, dann auch $B$ & $A \Rightarrow B \Leftrightarrow A \subset B$\\
\end{tabular}
Sind die Ereignisse $B_i$ paarweise verschieden, dann lässt sich $A$ in Teilergebnisse $B_i$ zerlegen 


\subsection{Axiomatische Wahrscheinlichkeit (Kolmogoroff)}
\begin{enumerate}\itemsep0em
	\item Axiom: Für jedes Ereignis $A$ gilt: $0 \leq P(A) \leq 1$
	\item Axiom (Normiertheit): Wenn $\Omega$ das Ereignis ist, dass alle Elementarergebnisse umfasst: $P(\Omega) = 1$
	\item Axiom (Additionstheorem): Schliessen sich $A$ und $B$ gegenseitig aus: $P(A \cup B) = P(A) + P(B)$ 
\end{enumerate}

\subsection{Bernulli Experiment}
Voraussetzungen:
\begin{itemize}\itemsep0em
	\item Die Wahrscheinlichkeit für jedes Ereignis ist stets gleich gross
	\item Jeder Versuch ist unabhängig von allen anderen Versuchen
\end{itemize}
Dann ist die Wahrscheinlichkeit für eine Serie $E_1E_2E_{\dots}E_n$ das Produkt der Einzelwahrscheinlichkeiten:
\begin{equation*}
	P(E_1E_2E_{\dots}E_n) = P(E_1) \cdot P(E_2) \cdot \dots \cdot P(E_n)
\end{equation*}
Beispiele sind etwa Würfeln oder Roulette (10 mal hintereinander Rot)

\subsection{Sätze}
\subsubsection{Unabhängigkeit von Ereignissen}
Zwei Ereignisse sind stochastisch unabhängig wenn gilt: $P(A|B) = P(A|\overline{B}) = P(A)$
d.\,h. es ist $A$ egal, ob $B$ eintritt.

\subsubsection{Additionssatz}
\begin{description}
	\item [Zwei beliebige Ereignisse] $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
	\item [Zwei sich ausschliessende Ereignisse] $P(A \cup B) = P(A) + P(B)$
	\item [Drei beliebige Ereignisse]
	\begin{align*}
			P(A \cup B \cup C)& = P(A) + P(B) + P(C) + P(A \cap B \cap C)\\
			& - P(A \cap B) - P(A \cap C) - P(B \cap C)
	\end{align*}
\end{description}

\subsection{Multiplikationssatz}
\begin{description}
	\item [Zwei beliebige Ereignisse] $P(A \cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)$
	\item [Zwei unabhängige Ereignisse] $P(A \cap B) = P(A) \cdot P(B)$
\end{description}

\subsubsection{Bedingte Wahrscheinlichkeit}
Bedingte Wahrscheinlichkeit (konditionale Wahrscheinlichkeit) $P(A|B)$ ist die Wahrscheinlichkeit des Eintretens eines Ereignisses $A$ unter der Bedingung (auch Konditionalität), dass das Eintreten eines anderen Ereignisses  $B$ bereits bekannt ist.
\begin{equation*}
	P(A\mid B) = \frac{P(A\cap B)}{P(B)}
\end{equation*}
wobei $P(A\cap B)$ die Wahrscheinlichkeit dafür ist, dass $A$ und $B$ gemeinsam auftreten.

\textbf{Beipiel:} 
\begin{enumerate}\itemsep0em
	\item In einer Urne (Bag 1) befinden sich 4 weisse ($W$) und 3 schwarze ($B$) Kugeln.
	\item Wir ziehen eine Kugel raus, dann befinden sich entweder noch 3 weisse und 3 schwarze Kugeln in der Urne (Bag 2)
	\item Wir ziehen wieder eine Kugel raus \dots
\end{enumerate}

% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=2.25cm, sibling distance=2.25cm]
\tikzstyle{level 2}=[level distance=2.25cm, sibling distance=1.25cm]

% Define styles for bags and leafs
\tikzstyle{bag} = [text width=4em, text centered]
\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]

% The sloped option gives rotated edge labels. Personally
% I find sloped labels a bit difficult to read. Remove the sloped options
% to get horizontal labels. 
\begin{tikzpicture}[grow=right, sloped]
\node[bag] {Bag 1 $4W, 3B$}
    child {
        node[bag] {Bag 2 $3W, 3B$}        
            child {
                node[end, label=right:
                    {$P(W_1\cap W_2)=\frac{4}{7}\cdot\frac{3}{6}$}] {}
                edge from parent
                node[above] {$W$}
                node[below]  {$\frac{3}{6}$}
            }
            child {
                node[end, label=right:
                    {$P(W_1\cap B_2)=\frac{4}{7}\cdot\frac{3}{6}$}] {}
                edge from parent
                node[above] {$B$}
                node[below]  {$\frac{3}{6}$}
            }
            edge from parent 
            node[above] {$W$}
            node[below]  {$\frac{4}{7}$}
    }
    child {
        node[bag] {Bag 2 $4W, 2B$}        
        child {
                node[end, label=right:
                    {$P(B_1\cap W_2)=\frac{3}{7}\cdot\frac{2}{6}$}] {}
                edge from parent
                node[above] {$B$}
                node[below]  {$\frac{2}{6}$}
            }
            child {
                node[end, label=right:
                    {$P(B_1\cap B_2)=\frac{3}{7}\cdot\frac{4}{6}$}] {}
                edge from parent
                node[above] {$W$}
                node[below]  {$\frac{4}{6}$}
            }
        edge from parent         
            node[above] {$B$}
            node[below]  {$\frac{3}{7}$}
    };
\end{tikzpicture}


\subsubsection{Totale Wahrscheinlichkeit}
Sind nur bedingte Wahrscheinlichkeiten und die Wahrscheinlichkeiten des bedingenden Ereignisses bekannt, ergibt sich die totale Wahrscheinlichkeit von $A$ aus:
\begin{equation*}
	P(A) = P\left(A\mid B\right)\cdot P(B)+P\left(A\mid\overline B\right)\cdot P\left(\overline B\right)
\end{equation*}
Anders ausgedrückt sind die Ereignisse $A_i$ paarweise verschieden und der $\Omega = A_1 \cup A_2 \cup \dots \cup A_n$ dann gilt für das Ereignis $E$:
\begin{equation*}
	P(E) = \sum_{i=1}^n P(E | A_i) \cdot P(A_i)
\end{equation*}
\textbf{Beispiel} Gegeben seien zwei Anlagen ($A_1$ und $A_2$). $A_1$ produziert 20\% Ausschuss $E$ ($0.2 = P(E|A_1)$), $A_2$ produziert 10\% Ausschuss ($0.1 = P(E|A_2)$).
Zwei drittel der Produktion finden auf $A_1$ statt ($P(A_1) = 2/3$). Wie hoch ist die Wahrscheinlichkeit, dass ein Teil Ausschuss ist?
$P(E) = P(E|A_1) \cdot P(A_1) + P(E|A_2) \cdot P(A_2) = 0.2 \cdot 2/3 + 0.1 \cdot 1/3 \approx 0{,}167$

\subsection{Theorem von Bayes}
Ausgangslage: $P($Wirkung$|$Ursache$)$ ist meist recht einfach. Interessant ist aber $P($Ursache$|$Ereignis$)$.
\begin{equation*}
	P(A_j|E) = \frac{P(A_j) \cdot P(E | A_j)}{\sum_{i=1}^n P(A_i) \cdot P(E|A_i)}
\end{equation*}
\textbf{Beispiel}: Wie hoch ist die Wahrscheinlichkeit, dass $A_j$ die Ursache für $E$ ist? (Wir haben ein defektes Teil in der Hand):
$P(A_1|E) = \frac{2/3 \cdot 0.2}{2/3 \cdot 0.2 + 1/3 \cdot 0.1} = \frac{4}{5}$: 80\% für Anlage $A_1$.

\subsection{Unterscheidung der Ansätze}
\begin{tabular}{@{}p{\linewidth/2-1em}%
				@{}|p{\linewidth/2-1em}}
	\multicolumn{1}{c}{\textbf{Frequentistischer Ansatz}} & \multicolumn{1}{c}{\textbf{Bayescher Ansatz}}\\\hline
	\textbullet~A-posterio Ansatz	& \textbullet~A-priori Ansatz\\
	\textbullet~beliebige Wiederholbarkeit & \textbullet~Kolmogoroff-Axiome\\
	\textbullet~relativen Häufigkeiten & \textbullet~bedingte Wahrscheinlichkeit\\
										&\textbullet~Aussagen über unwiederholbare Ereignisse\\
	Exakte Anworten auf uninteressante Frage & Unexakte Antworten auf interessante Fragen
\end{tabular}


